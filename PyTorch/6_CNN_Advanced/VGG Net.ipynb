{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG Net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alohasprit/TIL/blob/master/PyTorch/6_CNN_Advanced/VGG%20Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jrBujk0kYkco",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### VGG Net\n",
        "\n",
        "[1. VGG Net 논문](https://arxiv.org/pdf/1409.1556.pdf)\n",
        "2. 2014 ILSVRC 2nd Place\n",
        "3. VGG -1 16\n",
        "4. Convolution Layer\n",
        "5. Maxpooling Layer\n",
        "6. Fully Connected Layer\n",
        "![image](https://qph.fs.quoracdn.net/main-qimg-83c7dee9e8b039c3ca27c8dd91cacbb4)"
      ]
    },
    {
      "metadata": {
        "id": "1eDfTsq9Ykcr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Setting"
      ]
    },
    {
      "metadata": {
        "id": "tQMbblS4Ykcr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1-1. Import Required Libraries"
      ]
    },
    {
      "metadata": {
        "id": "BsEkrkStYkcs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch  \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.utils.data as data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8hIDUMOAYkcw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1-2 Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "Xq1dmkfJYkcx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "learning_rate = 0.001\n",
        "num_epoch = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "lu6TG1u0Ykcy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Data"
      ]
    },
    {
      "metadata": {
        "id": "RkQvdgxAYkcz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2-1 Download"
      ]
    },
    {
      "metadata": {
        "id": "1yVARcZUYkcz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "857a6b98-ccdb-4d37-e992-3b9b3b2c44f5"
      },
      "cell_type": "code",
      "source": [
        "img_dir = './CIFAR_Data'\n",
        "train = dset.CIFAR10(img_dir, train=True, target_transform = None, download=True,\n",
        "                        transform = transforms.Compose([\n",
        "                         transforms.Scale(256),\n",
        "                         transforms.RandomSizedCrop(224),\n",
        "                         transforms.RandomHorizontalFlip(),\n",
        "                        transforms.ToTensor(),\n",
        "                     ]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:563: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
            "  \"please use transforms.RandomResizedCrop instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./CIFAR_Data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kmq_kV50Ykc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea71d630-d09a-4b86-fc20-30f2d5fa9d75"
      },
      "cell_type": "code",
      "source": [
        "len(train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "-JJFy1U0Ykc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c5d52b6-f7da-4918-b079-909405c7c1f6"
      },
      "cell_type": "code",
      "source": [
        "train.__getitem__(0)[0].size()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 224, 224])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "9U5AUKHrYkc9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2-2. Data Batch"
      ]
    },
    {
      "metadata": {
        "id": "ZpjC-T0pYkc-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle = True, num_workers=2, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pkGcUvawYkdB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Model"
      ]
    },
    {
      "metadata": {
        "id": "W-vVrSJMYkdC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3-1 Basic Block"
      ]
    },
    {
      "metadata": {
        "id": "M64i1KIUBMQg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "d950OU3DYkdC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Conv_2_block\n",
        "def Conv_2_block(in_channel, out_channel):\n",
        "    model=nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "#Conv_3_block\n",
        "def Conv_3_block(in_channel, out_channel):\n",
        "    model = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "    )\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2JHxsedYkdE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3-2 VGG Net"
      ]
    },
    {
      "metadata": {
        "id": "MtNDDTxLYkdF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "fc5f8497-2771-41cd-f621-a7c1311f4f87"
      },
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    \n",
        "    def __init__(self, base_dim, num_classes=10):\n",
        "        super(VGG, self).__init__()\n",
        "        self.Conv = nn.Sequential(\n",
        "                                Conv_2_block(3, base_dim),\n",
        "                                Conv_2_block(base_dim, 2*base_dim),\n",
        "                                Conv_3_block(2*base_dim, 4*base_dim),\n",
        "                                Conv_3_block(4*base_dim, 8*base_dim),\n",
        "                                Conv_3_block(8*base_dim, 8*base_dim),\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "                                nn.Linear(8*base_dim*7*7, 100),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(),\n",
        "                                nn.Linear(100, 20),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(),\n",
        "                                nn.Linear(20, 10),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.Conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layer(x)\n",
        "        return x\n",
        "    \n",
        "model = VGG(base_dim=64).cuda()\n",
        "\n",
        "for i in model.named_children():\n",
        "    print(i) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Conv', Sequential(\n",
            "  (0): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (4): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "))\n",
            "('fc_layer', Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=100, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5)\n",
            "  (3): Linear(in_features=100, out_features=20, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5)\n",
            "  (6): Linear(in_features=20, out_features=10, bias=True)\n",
            "))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Ephw7oiYkdK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Optimizer & Loss"
      ]
    },
    {
      "metadata": {
        "id": "k4fASfHyYkdL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A6dcpxeJYkdN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5. Train"
      ]
    },
    {
      "metadata": {
        "id": "32xd1OHgYkdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "0d3b747a-267f-48b0-bf0c-feba07e6e386"
      },
      "cell_type": "code",
      "source": [
        "Loss =[]\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    for i, data in enumerate(train_loader):\n",
        "        images, labels = data\n",
        "        images = Variable(images).cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(images)\n",
        "        loss=loss_func(y_pred, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        Loss.append(loss)\n",
        "            \n",
        "        if i % 100 == 0:\n",
        "            print(epoch, i, loss)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 tensor(2.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 100 tensor(2.3235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 200 tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 300 tensor(2.2683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 400 tensor(2.2546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 500 tensor(2.3146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 600 tensor(2.3481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 700 tensor(2.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 800 tensor(2.3121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 900 tensor(2.2819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 1000 tensor(2.3496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 1100 tensor(2.3203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 1200 tensor(2.3115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 1300 tensor(2.2764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 1400 tensor(2.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 1500 tensor(2.2918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 1600 tensor(2.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 1700 tensor(2.2934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 1800 tensor(2.3208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 1900 tensor(2.2944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 2000 tensor(2.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 2100 tensor(2.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 2200 tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 2300 tensor(2.2871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 2400 tensor(2.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 2500 tensor(2.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 2600 tensor(2.3214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 2700 tensor(2.2913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 2800 tensor(2.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 2900 tensor(2.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 3000 tensor(2.3168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 3100 tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 3200 tensor(2.3167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 3300 tensor(2.3149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 3400 tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 3500 tensor(2.2907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 3600 tensor(2.2934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 3700 tensor(2.2912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 3800 tensor(2.3345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 3900 tensor(2.3314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 4000 tensor(2.2931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 4100 tensor(2.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 4200 tensor(2.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 4300 tensor(2.2900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 4400 tensor(2.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 4500 tensor(2.3276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 4600 tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 4700 tensor(2.3078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 4800 tensor(2.2961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 4900 tensor(2.2877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 5000 tensor(2.2697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 5100 tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 5200 tensor(2.3167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 5300 tensor(2.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 5400 tensor(2.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 5500 tensor(2.2857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 5600 tensor(2.3103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 5700 tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 5800 tensor(2.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 5900 tensor(2.3255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 6000 tensor(2.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 6100 tensor(2.2931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 6200 tensor(2.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 6300 tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 6400 tensor(2.3224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 6500 tensor(2.2928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 6600 tensor(2.2996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 6700 tensor(2.3333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 6800 tensor(2.2782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 6900 tensor(2.2917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 7000 tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 7100 tensor(2.3103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 7200 tensor(2.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 7300 tensor(2.3140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 7400 tensor(2.3207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 7500 tensor(2.3144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 7600 tensor(2.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 7700 tensor(2.3106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 7800 tensor(2.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 7900 tensor(2.3294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 8000 tensor(2.3151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 8100 tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 8200 tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 8300 tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 8400 tensor(2.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 8500 tensor(2.2809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 8600 tensor(2.2996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 8700 tensor(2.3140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 8800 tensor(2.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 8900 tensor(2.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 9000 tensor(2.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 9100 tensor(2.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 9200 tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 9300 tensor(2.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 9400 tensor(2.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 9500 tensor(2.2899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 9600 tensor(2.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 9700 tensor(2.3143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 9800 tensor(2.3088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "0 9900 tensor(2.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zh8L7hmhYkdR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}