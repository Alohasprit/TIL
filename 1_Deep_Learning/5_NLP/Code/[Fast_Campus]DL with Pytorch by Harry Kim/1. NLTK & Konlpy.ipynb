{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. NLTK & Konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1 NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/hoyounson/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hoyounson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/hoyounson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hoyounson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "# https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
    "# http://www.nltk.org/nltk_data/\n",
    "\n",
    "nltk.download(\"gutenberg\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.corpus.gutenberg.raw(\"shakespeare-macbeth.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Thunder and Lightning. Enter three Witches.\n",
      "\n",
      "  1. When shall we three meet againe?\n",
      "In Thunder, Lightning, or in Raine?\n",
      "  2. When the Hurley-burley's done,\n",
      "When the Battaile's lost, and wonne\n",
      "\n",
      "   3. That will be ere the set of Sunne\n",
      "\n",
      "   1. Where the place?\n",
      "  2. Vpon the Heath\n",
      "\n",
      "   3. There to meet with Macbeth\n",
      "\n",
      "   1. I come, Gray-Malkin\n",
      "\n",
      "   All. Padock calls anon: faire is foule, and foule is faire,\n",
      "Houer through \n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'the', 'tragedie', 'of', 'macbeth', 'by', 'william', 'shakespeare', '1603', ']', 'actus', 'primus', '.', 'scoena', 'prima', '.', 'thunder', 'and', 'lightning', '.', 'enter', 'three', 'witches', '.', '1.', 'when', 'shall', 'we', 'three', 'meet', 'againe', '?', 'in', 'thunder', ',', 'lightning', ',', 'or', 'in', 'raine', '?', '2.', 'when', 'the', 'hurley-burley', \"'s\", 'done', ',', 'when', 'the', 'battaile', \"'s\", 'lost', ',', 'and', 'wonne', '3.', 'that', 'will', 'be', 'ere', 'the', 'set', 'of', 'sunne', '1.', 'where', 'the', 'place', '?', '2.', 'vpon', 'the', 'heath', '3.', 'there', 'to', 'meet', 'with', 'macbeth', '1.', 'i', 'come', ',', 'gray-malkin', 'all', '.', 'padock', 'calls', 'anon', ':', 'faire', 'is', 'foule', ',', 'and', 'foule', 'is', 'faire', ',', 'houer', 'through']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'tragedie', 'of', 'macbeth', 'by', 'william', 'shakespeare', '1603', 'actus', 'primus', 'scoena', 'prima', 'thunder', 'and', 'lightning', 'enter', 'three', 'witches', '1', 'when', 'shall', 'we', 'three', 'meet', 'againe', 'in', 'thunder', 'lightning', 'or', 'in', 'raine', '2', 'when', 'the', 'hurley', 'burley', 's', 'done', 'when', 'the', 'battaile', 's', 'lost', 'and', 'wonne', '3', 'that', 'will', 'be', 'ere', 'the', 'set', 'of', 'sunne', '1', 'where', 'the', 'place', '2', 'vpon', 'the', 'heath', '3', 'there', 'to', 'meet', 'with', 'macbeth', '1', 'i', 'come', 'gray', 'malkin', 'all', 'padock', 'calls', 'anon', 'faire', 'is', 'foule', 'and', 'foule', 'is', 'faire', 'houer', 'through']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "# 정규 표현식\n",
    "# https://ko.wikipedia.org/wiki/%EC%A0%95%EA%B7%9C_%ED%91%9C%ED%98%84%EC%8B%9D\n",
    "tokens = RegexpTokenizer(\"[\\w]+\").tokenize(text[:500])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tragedie', 'macbeth', 'william', 'shakespeare', '1603', 'actus', 'primus', 'scoena', 'prima', 'thunder', 'lightning', 'enter', 'three', 'witches', '1', 'shall', 'three', 'meet', 'againe', 'thunder', 'lightning', 'raine', '2', 'hurley', 'burley', 'done', 'battaile', 'lost', 'wonne', '3', 'ere', 'set', 'sunne', '1', 'place', '2', 'vpon', 'heath', '3', 'meet', 'macbeth', '1', 'come', 'gray', 'malkin', 'padock', 'calls', 'anon', 'faire', 'foule', 'foule', 'faire', 'houer']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopping = set(stopwords.words('english'))\n",
    "print([token for token in tokens if not token in stopping])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'the' in stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'tragedi',\n",
       " 'of',\n",
       " 'macbeth',\n",
       " 'by',\n",
       " 'william',\n",
       " 'shakespear',\n",
       " '1603',\n",
       " 'actu',\n",
       " 'primu',\n",
       " 'scoena',\n",
       " 'prima',\n",
       " 'thunder',\n",
       " 'and',\n",
       " 'lightn',\n",
       " 'enter',\n",
       " 'three',\n",
       " 'witch',\n",
       " '1',\n",
       " 'when',\n",
       " 'shall',\n",
       " 'we',\n",
       " 'three',\n",
       " 'meet',\n",
       " 'again',\n",
       " 'in',\n",
       " 'thunder',\n",
       " 'lightn',\n",
       " 'or',\n",
       " 'in',\n",
       " 'rain',\n",
       " '2',\n",
       " 'when',\n",
       " 'the',\n",
       " 'hurley',\n",
       " 'burley',\n",
       " 's',\n",
       " 'done',\n",
       " 'when',\n",
       " 'the',\n",
       " 'battail',\n",
       " 's',\n",
       " 'lost',\n",
       " 'and',\n",
       " 'wonn',\n",
       " '3',\n",
       " 'that',\n",
       " 'will',\n",
       " 'be',\n",
       " 'ere',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'sunn',\n",
       " '1',\n",
       " 'where',\n",
       " 'the',\n",
       " 'place',\n",
       " '2',\n",
       " 'vpon',\n",
       " 'the',\n",
       " 'heath',\n",
       " '3',\n",
       " 'there',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'with',\n",
       " 'macbeth',\n",
       " '1',\n",
       " 'i',\n",
       " 'come',\n",
       " 'gray',\n",
       " 'malkin',\n",
       " 'all',\n",
       " 'padock',\n",
       " 'call',\n",
       " 'anon',\n",
       " 'fair',\n",
       " 'is',\n",
       " 'foul',\n",
       " 'and',\n",
       " 'foul',\n",
       " 'is',\n",
       " 'fair',\n",
       " 'houer',\n",
       " 'through']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "[PorterStemmer().stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('tragedie', 'NN'), ('of', 'IN'), ('macbeth', 'NN'), ('by', 'IN'), ('william', 'NN'), ('shakespeare', 'NN'), ('1603', 'CD'), ('actus', 'NN'), ('primus', 'NN'), ('scoena', 'NN'), ('prima', 'NN'), ('thunder', 'NN'), ('and', 'CC'), ('lightning', 'NN'), ('enter', 'NN'), ('three', 'CD'), ('witches', 'NNS'), ('1', 'CD'), ('when', 'WRB'), ('shall', 'MD'), ('we', 'PRP'), ('three', 'CD'), ('meet', 'NNS'), ('againe', 'VBP'), ('in', 'IN'), ('thunder', 'NN'), ('lightning', 'NN'), ('or', 'CC'), ('in', 'IN'), ('raine', 'JJ'), ('2', 'CD'), ('when', 'WRB'), ('the', 'DT'), ('hurley', 'NN'), ('burley', 'NN'), ('s', 'NN'), ('done', 'VBN'), ('when', 'WRB'), ('the', 'DT'), ('battaile', 'NN'), ('s', 'NN'), ('lost', 'VBN'), ('and', 'CC'), ('wonne', '$'), ('3', 'CD'), ('that', 'WDT'), ('will', 'MD'), ('be', 'VB'), ('ere', 'RB'), ('the', 'DT'), ('set', 'NN'), ('of', 'IN'), ('sunne', 'NN'), ('1', 'CD'), ('where', 'WRB'), ('the', 'DT'), ('place', 'NN'), ('2', 'CD'), ('vpon', 'IN'), ('the', 'DT'), ('heath', 'NN'), ('3', 'CD'), ('there', 'RB'), ('to', 'TO'), ('meet', 'VB'), ('with', 'IN'), ('macbeth', 'JJ'), ('1', 'CD'), ('i', 'JJ'), ('come', 'VBP'), ('gray', 'JJ'), ('malkin', 'FW'), ('all', 'DT'), ('padock', 'NN'), ('calls', 'VBZ'), ('anon', 'JJ'), ('faire', 'NN'), ('is', 'VBZ'), ('foule', 'JJ'), ('and', 'CC'), ('foule', 'NN'), ('is', 'VBZ'), ('faire', 'JJ'), ('houer', 'NN'), ('through', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(tagger, doc):\n",
    "    return [\"/\".join(p) for p in tagger(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the/DT',\n",
       " 'tragedie/NN',\n",
       " 'of/IN',\n",
       " 'macbeth/NN',\n",
       " 'by/IN',\n",
       " 'william/NN',\n",
       " 'shakespeare/NN',\n",
       " '1603/CD',\n",
       " 'actus/NN',\n",
       " 'primus/NN',\n",
       " 'scoena/NN',\n",
       " 'prima/NN',\n",
       " 'thunder/NN',\n",
       " 'and/CC',\n",
       " 'lightning/NN',\n",
       " 'enter/NN',\n",
       " 'three/CD',\n",
       " 'witches/NNS',\n",
       " '1/CD',\n",
       " 'when/WRB',\n",
       " 'shall/MD',\n",
       " 'we/PRP',\n",
       " 'three/CD',\n",
       " 'meet/NNS',\n",
       " 'againe/VBP',\n",
       " 'in/IN',\n",
       " 'thunder/NN',\n",
       " 'lightning/NN',\n",
       " 'or/CC',\n",
       " 'in/IN',\n",
       " 'raine/JJ',\n",
       " '2/CD',\n",
       " 'when/WRB',\n",
       " 'the/DT',\n",
       " 'hurley/NN',\n",
       " 'burley/NN',\n",
       " 's/NN',\n",
       " 'done/VBN',\n",
       " 'when/WRB',\n",
       " 'the/DT',\n",
       " 'battaile/NN',\n",
       " 's/NN',\n",
       " 'lost/VBN',\n",
       " 'and/CC',\n",
       " 'wonne/$',\n",
       " '3/CD',\n",
       " 'that/WDT',\n",
       " 'will/MD',\n",
       " 'be/VB',\n",
       " 'ere/RB',\n",
       " 'the/DT',\n",
       " 'set/NN',\n",
       " 'of/IN',\n",
       " 'sunne/NN',\n",
       " '1/CD',\n",
       " 'where/WRB',\n",
       " 'the/DT',\n",
       " 'place/NN',\n",
       " '2/CD',\n",
       " 'vpon/IN',\n",
       " 'the/DT',\n",
       " 'heath/NN',\n",
       " '3/CD',\n",
       " 'there/RB',\n",
       " 'to/TO',\n",
       " 'meet/VB',\n",
       " 'with/IN',\n",
       " 'macbeth/JJ',\n",
       " '1/CD',\n",
       " 'i/JJ',\n",
       " 'come/VBP',\n",
       " 'gray/JJ',\n",
       " 'malkin/FW',\n",
       " 'all/DT',\n",
       " 'padock/NN',\n",
       " 'calls/VBZ',\n",
       " 'anon/JJ',\n",
       " 'faire/NN',\n",
       " 'is/VBZ',\n",
       " 'foule/JJ',\n",
       " 'and/CC',\n",
       " 'foule/NN',\n",
       " 'is/VBZ',\n",
       " 'faire/JJ',\n",
       " 'houer/NN',\n",
       " 'through/IN']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(pos_tag, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2 Konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy\n",
    "from konlpy.tag import Twitter as twit\n",
    "# https://konlpy-ko.readthedocs.io/ko/v0.4.3/api/konlpy.tag/#module-konlpy.tag._twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoyounson/anaconda3/lib/python3.7/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Package kr.lucypark.okt.OktInterface is not Callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-664a62c0696a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/konlpy/tag/_okt.py\u001b[0m in \u001b[0;36mTwitter\u001b[0;34m(jvmpath)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mOkt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvmpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/konlpy/tag/_okt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, jvmpath, max_heap_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0moktJavaPackage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjpype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJPackage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kr.lucypark.okt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mOktInterfaceJavaClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moktJavaPackage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOktInterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOktInterfaceJavaClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/data/tagset/twitter.json'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstallpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jpype/_jpackage.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *arg, **kwarg)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Package {0} is not Callable\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Package kr.lucypark.okt.OktInterface is not Callable"
     ]
    }
   ],
   "source": [
    "tagger = twit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0f7aa3a44e38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"역시 파이토치는 재미있네요 ㅋㅋㅋ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tagger' is not defined"
     ]
    }
   ],
   "source": [
    "print(tagger.pos(\"역시 파이토치는 재미있네요 ㅋㅋㅋ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tagger.nouns(\"사과는 맛있지만, 바나나는 맛없다\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tagger.pos(\"아니, 이렇게 신기할수가?\", stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(tagger.pos, \"사랑하는 자여 네 영혼이 잘됨 같이 네가 범사에 잘되고 강건하기를\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.3 Other Tools For NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawWordCloud(data):\n",
    "    wordcloud = WordCloud(stopwords = STOPWORDS, \n",
    "                          background_color = 'white', width= 800, height = 400).generate(data)\n",
    "    plt.figure(figsize = (15 , 12))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DrawWordCloud(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = dict(Counter(RegexpTokenizer(\"[\\w]+\").tokenize(text)).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Zip keys = labels / values = values\n",
    "labels, values = zip(*counts.items())\n",
    "\n",
    "indSort = np.argsort(values)[::-1]\n",
    "\n",
    "labels = np.array(labels)[indSort]\n",
    "values = np.array(values)[indSort]\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.xlabel('Top 15 Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Word Frequency')\n",
    "\n",
    "plt.bar(indexes, values)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(values)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
